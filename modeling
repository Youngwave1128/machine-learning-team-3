import pandas as pd
import numpy as np
import sys
import traceback
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt # [신규] 시각화
import seaborn as sns             # [신규] 시각화
try:
    from kmodes.kprototypes import KPrototypes # K-Prototypes 임포트
except ImportError:
    print("[알림] 'kmodes' 라이브러리를 설치합니다.")
    print("설치 완료 후 이 셀을 다시 실행해주세요.")
    # !pip install kmodes
    # sys.exit()

# --- 0. 설정 ---

K_CLUSTERS = 10

CLUSTER_FEATURES = ['height_cm', 'size_scaled_0_to_10', 'bra size', 'cup size']
NUMERICAL_FEATURES = ['height_cm', 'size_scaled_0_to_10', 'bra size']
CATEGORICAL_FEATURES = ['cup size']

in_train_file = "renttherunway_processed_final.csv"
in_test_file = "modcloth_processed_final.csv"

out_train_file = "renttherunway_clustered.csv"
out_test_file = "modcloth_clustered.csv"

print(f"--- 2단계: [수정] 유저 그룹화 (K-Prototypes, bra size=Numerical) 시작 ---")

try:
    # --- 1. 훈련 데이터 로드 및 전처리 ---
    print(f"훈련 데이터 '{in_train_file}' 로드 중...")
    df_train = pd.read_csv(in_train_file)
    df_train.columns = df_train.columns.str.strip()
    print(f"훈련 데이터 {len(df_train):,}건 로드 완료.")

    print("--- 훈련 데이터 전처리 (결측치 처리 및 스케일링) ---")

    # 1a. 클러스터링용 데이터 추출 (원본 복사)
    df_train_cluster_data = df_train[CLUSTER_FEATURES].copy()

    # 1b. 범주형 데이터 타입 변환 (str) - 'cup size'만
    df_train_cluster_data[CATEGORICAL_FEATURES] = df_train_cluster_data[CATEGORICAL_FEATURES].astype(str)

    # 1c. 숫자형 데이터 타입 변환 (float) - 'bra size' 포함
    for col in NUMERICAL_FEATURES:
        df_train_cluster_data[col] = pd.to_numeric(df_train_cluster_data[col], errors='coerce')

    # 1d. 숫자형 데이터 결측치 처리 (SimpleImputer)
    num_imputer = SimpleImputer(strategy='median')
    df_train_cluster_data[NUMERICAL_FEATURES] = num_imputer.fit_transform(
        df_train_cluster_data[NUMERICAL_FEATURES]
    )

    # 1e. 숫자형 데이터 스케일링 (StandardScaler)
    scaler = StandardScaler()
    df_train_cluster_data[NUMERICAL_FEATURES] = scaler.fit_transform(
        df_train_cluster_data[NUMERICAL_FEATURES]
    )

    # 1f. K-Prototypes 입력용 Matrix 생성
    train_matrix = df_train_cluster_data.values

    # 1g. 범주형 데이터의 컬럼 인덱스 찾기
    categorical_indices = [df_train_cluster_data.columns.get_loc(c) for c in CATEGORICAL_FEATURES]

    print(f"전처리 완료된 데이터 {len(train_matrix):,}건 준비.")
    print(f"숫자형 피처: {NUMERICAL_FEATURES}")
    print(f"범주형 피처: {CATEGORICAL_FEATURES} (인덱스: {categorical_indices})")


    # --- 2. 최종 모델 훈련 (K=10) ---
    print(f"\n--- 최종 모델 훈련 (K={K_CLUSTERS}) 시작 ---")

    kproto_final = KPrototypes(
        n_clusters=K_CLUSTERS,
        init='Huang',
        n_init=10,
        random_state=42,
        verbose=1,
        n_jobs=-1
    )

    clusters_train = kproto_final.fit_predict(train_matrix, categorical=categorical_indices)
    print("최종 모델 훈련 완료.")


    # --- [신규] 2a. 시각화 섹션 ---
    print("\n--- [신규] 2a. 클러스터링 결과 시각화 ---")

    # 원본 데이터(df_train)에 클러스터 ID 임시 할당 (시각화용)
    df_train.loc[df_train_cluster_data.index, 'cluster_id'] = clusters_train
    # (결측치 등으로 클러스터링에서 제외된 유저는 cluster_id가 NaN일 것임)
    df_train_viz = df_train.dropna(subset=['cluster_id'])
    df_train_viz['cluster_id'] = df_train_viz['cluster_id'].astype(int)

    # 시각화 1: 클러스터별 인원 수 (Bar Chart)
    plt.figure(figsize=(10, 6))
    sns.countplot(data=df_train_viz, x='cluster_id', palette='tab10')
    plt.title(f'Cluster Size Distribution (K={K_CLUSTERS})', fontsize=15)
    plt.xlabel('Cluster ID')
    plt.ylabel('Number of Users')
    plt.show()


    # --- 3. 훈련 데이터에 클러스터 및 Context 적용 ---
    print("\n--- 3. 훈련 데이터에 'context' 적용 ---")

    # (시각화에서 이미 cluster_id를 할당했지만, 혹시 모를 전체 데이터 대상으로 한번 더 실행)
    df_train.loc[df_train_cluster_data.index, 'cluster_id'] = clusters_train

    df_train['cluster_id'] = df_train['cluster_id'].astype(float).astype('Int64')
    df_train['context'] = 'cluster_' + df_train['cluster_id'].astype(str) + '_' + df_train['rented for']

    print(f"훈련 데이터 'cluster_id' 및 'context' 생성 완료.")

    df_train.to_csv(out_train_file, index=False, encoding='utf-8-sig')
    print(f"✅ 훈련 데이터 저장 완료 -> {out_train_file}")


    # --- 4. 테스트 데이터(Test) 준비 및 예측 ---
    print(f"\n--- 4. 테스트 데이터 전처리 및 예측 ---")

    df_test = pd.read_csv(in_test_file)
    df_test.columns = df_test.columns.str.strip()
    print(f"테스트 데이터 '{in_test_file}' 로드 ({len(df_test):,}건)")

    # [수정] 4b. '.dropna()' 제거
    df_test_cluster_data = df_test[CLUSTER_FEATURES].copy()

    # 4c. 범주형 데이터 타입 변환 (str) - 'cup size'만
    df_test_cluster_data[CATEGORICAL_FEATURES] = df_test_cluster_data[CATEGORICAL_FEATURES].astype(str)

    # 4d. 숫자형 데이터 타입 변환 (float) - 'bra size' 포함
    for col in NUMERICAL_FEATURES:
        df_test_cluster_data[col] = pd.to_numeric(df_test_cluster_data[col], errors='coerce')

    # 4e. 숫자형 데이터 결측치 처리 (훈련 Imputer로 transform)
    df_test_cluster_data[NUMERICAL_FEATURES] = num_imputer.transform(
        df_test_cluster_data[NUMERICAL_FEATURES]
    )

    # 4f. 숫자형 데이터 스케일링 (훈련 Scaler로 transform)
    df_test_cluster_data[NUMERICAL_FEATURES] = scaler.transform(
        df_test_cluster_data[NUMERICAL_FEATURES]
    )

    # 4g. K-Prototypes 입력용 Matrix 생성
    test_matrix = df_test_cluster_data.values

    # 4h. 테스트 데이터 예측
    clusters_test = kproto_final.predict(test_matrix, categorical=categorical_indices)
    print("테스트 데이터 예측 완료.")


    # --- 5. 테스트 데이터에 클러스터 및 Context 적용 ---

    df_test.loc[df_test_cluster_data.index, 'cluster_id'] = clusters_test

    df_test['cluster_id'] = df_test['cluster_id'].astype(float).astype('Int64')
    df_test['context'] = 'cluster_' + df_test['cluster_id'].astype(str) + '_' + df_test['rented for']

    print(f"테스트 데이터 'cluster_id' 및 'context' 생성 완료.")

    df_test.to_csv(out_test_file, index=False, encoding='utf-8-sig')
    print(f"✅ 테스트 데이터 저장 완료 -> {out_test_file}")

    print("\n--- 모든 작업 완료 ---")

except ImportError as e:
    print("[오류] 'kmodes' 라이브러리가 설치되지 않았습니다.")
    print("코랩 셀에서 '!pip install kmodes'를 실행해주세요.")
except FileNotFoundError as e:
    print(f"[파일 로드 오류] {e}.")
    print("1단계 출력 파일인 'renttherunway_processed_final.csv' 또는 'modcloth_processed_final.csv'가 필요합니다.")
except KeyError as e:
    print(f"[KeyError] {e}. 1단계 파일에 {CLUSTER_FEATURES} 컬럼이 모두 있는지 확인하세요.")
    print("   (참고: 1단계 스크립트가 'size_scaled_0_to_10'을 생성했는지 확인)")
except Exception as e:
    print(f"처리 중 예기치 않은 오류 발생: {e}")
    traceback.print_exc()
print("--- 3단계: [수정] CF Score (Rating + Fit + n_USERS) 모델 구축 ---")

# --- 1. 데이터 로드 ---
in_train_clustered = "renttherunway_clustered.csv"

try:
    df_train = pd.read_csv(in_train_clustered, dtype={'product_code': str, 'user_id': str})

    # 컬럼명 공백 제거
    df_train.columns = df_train.columns.str.strip()

    if 'fit' not in df_train.columns:
        print("[오류] 'renttherunway_clustered.csv'에서 'fit' 컬럼을 찾을 수 없습니다!")
        print(f"  > 찾은 컬럼: {df_train.columns.tolist()}")
        # sys.exit()

    print(f"클러스터링된 훈련 데이터 {len(df_train):,}건 로드 및 컬럼명 정제 완료.")

except FileNotFoundError as e:
    print(f"[오류] {e}. 'renttherunway_clustered.csv' 파일이 필요합니다.")
    # sys.exit()

# --- 2. Context 정의 ---
df_train['context'] = 'cluster_' + df_train['cluster_id'].astype(str) + \
                      '_rent_' + df_train['rented for'].astype(str)
print("Context 컬럼 생성 완료.")

# --- 3. 'rating'과 'fit'에 대한 신뢰도 보정 점수 계산 ---
print("Context-Item 별 '신뢰도 보정' 평점 (Rating + Fit) 계산 중...")

# 3a. 입력값 전처리
df_train['rating'] = pd.to_numeric(df_train['rating'], errors='coerce')

def map_fit_score(fit_value):
    if fit_value == 'fit':
        return 1
    elif fit_value in ['small', 'large']:
        return -1
    else:
        return 0 # (e.g., NaN 또는 'nan')

df_train['fit_numeric'] = df_train['fit'].apply(map_fit_score)
df_train = df_train.dropna(subset=['rating']) # 평점이 없는 리뷰는 제외

# 3b. 글로벌 평균(m) 및 신뢰도 상수(C) 정의
m_rating = df_train['rating'].mean()
m_fit = df_train['fit_numeric'].mean()
C = 5 # 신뢰도 상수

print(f"  글로벌 평균 평점(m_rating) = {m_rating:.2f}")
print(f"  글로벌 평균 핏(m_fit) = {m_fit:.2f}")
print(f"  신뢰도 상수(C) = {C}")

# 3c. (Context, Product) 그룹별로 '평균(R)'과 '고유 사용자 수(n)' 집계
df_agg = df_train.groupby(
    ['context', 'product_code']
).agg(
    # [수정] 'rating'의 개수가 아닌, 'user_id'의 고유(unique) 개수를 신뢰도(n)로 사용
    n_users=('user_id', 'nunique'), # 'n_reviews' -> 'n_users'
    R_rating=('rating', 'mean'),   # 'rating' 컬럼의 평균
    R_fit=('fit_numeric', 'mean')  # 'fit' 컬럼의 평균
).reset_index()

# 3d. 베이지안 스무딩 (신뢰도 보정) 공식 적용
# [수정] 'n' 변수가 'n_reviews'가 아닌 'n_users'를 사용하도록 변경
n = df_agg['n_users']
# Rating Score 보정
df_agg['cf_rating_score'] = ((C * m_rating) + (n * df_agg['R_rating'])) / (C + n)
# Fit Score 보정
df_agg['cf_fit_score'] = ((C * m_fit) + (n * df_agg['R_fit'])) / (C + n)

# 3e. [핵심 모델] 완성
# [수정] 'n_reviews' -> 'n_users'
cf_score_table = df_agg[['context', 'product_code', 'cf_rating_score', 'cf_fit_score', 'n_users']]

print("신뢰도 보정된 (Rating + Fit) 세부 평점 테이블(CF Score Model) 생성 완료.")


# --- 4. "그룹(Context)별" '전체' 평균 평점 "표"로 출력 ---
print("\n--- 4. '그룹별 전체 평균 평점' 요약 (표) ---")

context_overall_scores = cf_score_table.groupby('context')['cf_rating_score'].mean().reset_index()
context_overall_scores = context_overall_scores.rename(columns={'cf_rating_score': 'overall_avg_rating'})
context_scores_sorted = context_overall_scores.sort_values(by='overall_avg_rating', ascending=False)

print("\n[표 1] 평점이 가장 후한 그룹 (Top 10)")
print(context_scores_sorted.head(10).to_string())
print("\n[표 2] 평점이 가장 짠 그룹 (Bottom 10)")
print(context_scores_sorted.tail(10).to_string())


# --- 5. CF Score "세부" 모델 저장 ---
out_model_file = "cf_model_final.csv"
cf_score_table.to_csv(out_model_file, index=False, encoding='utf-8-sig')

print("\n--- 3단계 (CF) + 'Fit' + 'n_users' 완료 ---")
print(f"✅ '세부' 평점 모델 저장: {out_model_file} ({len(cf_score_table):,}줄)")
print("\n--- '세부' 평점 예시 데이터 (4단계에서 이 파일을 사용합니다) ---")
print(cf_score_table.head())
